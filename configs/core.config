import numpy as np
import torch
import torch.nn as nn
from amid.totalsegmentator import Totalsegmentator
from connectome import CacheToRam, Chain, Filter
from dpipe import layers
from dpipe.batch_iter import Infinite, Threads, apply_at, combine_pad, sample
from dpipe.im.metrics import dice_score, precision, recall
from dpipe.predict import divisible_shape
from dpipe.predict.shape import patches_grid
from dpipe.torch.functional import weighted_cross_entropy_with_logits
from lightning import Trainer
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger
from sklearn.model_selection import train_test_split
from thunder import ThunderModule
from thunder.callbacks import MetricLogger, TimeProfiler
from thunder.layout import Split
from thunder.placeholders import ExpName, GroupName
from thunder.policy import Switch
from torch.optim import Adam
from torch.utils.data import DataLoader

from thunder_examples.dataset import ConToTorch, NormalizeCT, RotateTotalsegm
from thunder_examples.dataset.augm import RandomPatch
from thunder_examples.predict import DecoratedPredictor, add_channels_dims

SEED = 0xBadCafe

totalsegmentator = Totalsegmentator("/shared/data/Totalsegmentator_dataset.zip") >> Filter(lambda study_type, split: study_type == "ct abdomen-pelvis" and split == "train")
preprocessing = Chain(RotateTotalsegm(), NormalizeCT(max_=150, min_=-50))

dataset = Chain(totalsegmentator,
                preprocessing,
                CacheToRam())


layout = Split(train_test_split, dataset, names=("train", "val"))


patch_size = np.asarray((164, 164, 64))
patch_stride = patch_size // 2

train_loader = (layout.train >> RandomPatch(patch_size))._compile(("image", "liver"))



batch_size = 2
batches_per_epoch = 256
max_epochs = 200


train_data = Infinite(
    sample(layout.train.ids, random_state=SEED),
    Threads(train_loader, n_workers=2),
    apply_at(index=[0, 1], func=np.float32),
    apply_at(index=[0, 1], func=lambda x: x[None]),
    batch_size=batch_size, batches_per_epoch=batches_per_epoch,
)

val_data = DataLoader(
    ConToTorch(layout.val, ['image', 'liver']), batch_size=1, collate_fn=combine_pad)


architecture = nn.Sequential(
    nn.Conv3d(1, 8, kernel_size=3, padding=1),

    layers.FPN(
        layers.ResBlock3d, nn.MaxPool3d(2), nn.Identity(),
        layers.fpn.interpolate_merge(lambda x, y: torch.cat([x, y], 1), order=1),
        [
            [[8, 16, 16], [32, 16, 8]],
            [[16, 32, 32], [64, 32, 16]],
            [[32, 64, 64], [128, 64, 32]],
            [[64, 128, 128], [256, 128, 64]],
            [[128, 256, 256], [512, 256, 128]],
            [256, 512, 256],
        ],
        kernel_size=3, padding=1,
    ),

    layers.PreActivation3d(8, 1, kernel_size=3, padding=1),
)


criterion = weighted_cross_entropy_with_logits


module = ThunderModule(architecture, criterion, nn.Sigmoid(),
                       optimizer=Adam(architecture.parameters()),
                        lr_scheduler=Switch({0: 1e-3, 50: 1e-4, 150: 1e-5}),
                        predictor=DecoratedPredictor(add_channels_dims(),
                                                     patches_grid(patch_size, patch_stride),
                                                     divisible_shape((32, 32, 32), padding_values=np.min)))

trainer = Trainer(
    callbacks=[
        MetricLogger({lambda x, y: (y, x > 0,5): [precision, recall, dice_score]}, aggregate_fn=["std", "max", "min"]),
        TimeProfiler(),
        LearningRateMonitor("epoch"),
        ModelCheckpoint(save_last=True),
    ],
    limit_train_batches=batches_per_epoch,
    accelerator='cuda', precision=16,
    max_epochs=max_epochs,
    logger=WandbLogger(name=ExpName, group=GroupName, project='thunder-examples', entity='arseniybelkov'))
